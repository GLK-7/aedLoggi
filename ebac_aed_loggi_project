{"metadata":{"colab":{"name":"module_17_exercise.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyO58SjEKvZru6fhTie9JEM/"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://raw.githubusercontent.com/andre-marcos-perez/ebac-course-utils/main/media/logo/newebac_logo_black_half.png\" alt=\"ebac-logo\">\n\n---\n\nCaderno de **Exercícios**<br> \nProfessor [André Perez](https://www.linkedin.com/in/andremarcosperez/)\n\n---","metadata":{"id":"KJqp9AANOCtf"}},{"cell_type":"markdown","source":"# **Tópicos**\n\n<ol type=\"1\">\n  <li>Manipulação;</li>\n  <li>Visualização;</li>\n  <li>Storytelling.</li>\n</ol>\n","metadata":{"id":"d9jDtUbDOE1-"}},{"cell_type":"markdown","source":"---","metadata":{"id":"zMN1Q3jdwoJm"}},{"cell_type":"markdown","source":"# **Análise Exploratória de Dados de Logística - Empresa Loggi**\n","metadata":{"id":"QRcqbpLpFK5o"}},{"cell_type":"markdown","source":"## 1\\. Contexto","metadata":{"id":"6-CvdKwqFPiW"}},{"cell_type":"markdown","source":"<img width=\"250px\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Imagem_Logo_Completo_Azul.png/800px-Imagem_Logo_Completo_Azul.png\" alt=\"loggi-logo\"> \n\nA Loggi é uma startup brasileira de tecnologia, que se tornou um unicórnio, especializada em logística. Iniciou suas atividades entregando documentos entre 2013 e 2014. Em 2016, expandiu seus serviços para o e-commerce e, a partir de 2017, passou a atuar também no segmento de entregas de alimentos.\n\n\nNeste projeto, realizaremos uma análise das entregas feitas pela empresa no Distrito Federal. Ao longo deste caderno, exploraremos, reorganizaremos e estruturaremos os dados para que possam ser visualizados de forma clara e compreensível. Dessa maneira, será possível extrair insights valiosos a partir das informações apresentadas.","metadata":{}},{"cell_type":"markdown","source":"## 2\\. Pacotes e bibliotecas","metadata":{"id":"QxukLHaqFnkU"}},{"cell_type":"code","source":"!pip3 install geopandas;","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:56:57.217831Z","iopub.execute_input":"2024-08-06T16:56:57.218248Z","iopub.status.idle":"2024-08-06T16:57:34.171150Z","shell.execute_reply.started":"2024-08-06T16:56:57.218214Z","shell.execute_reply":"2024-08-06T16:57:34.169395Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: geopandas in /opt/conda/lib/python3.10/site-packages (0.14.4)\nRequirement already satisfied: fiona>=1.8.21 in /opt/conda/lib/python3.10/site-packages (from geopandas) (1.9.6)\nRequirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from geopandas) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from geopandas) (21.3)\nRequirement already satisfied: pandas>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from geopandas) (2.2.2)\nRequirement already satisfied: pyproj>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from geopandas) (3.6.1)\nRequirement already satisfied: shapely>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from geopandas) (1.8.5.post1)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas) (23.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas) (2024.7.4)\nRequirement already satisfied: click~=8.0 in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas) (8.1.7)\nRequirement already satisfied: click-plugins>=1.0 in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas) (1.1.1)\nRequirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas) (0.7.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.4.0->geopandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.4.0->geopandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.4.0->geopandas) (2023.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->geopandas) (3.1.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip3 install geopy","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:58:06.457126Z","iopub.execute_input":"2024-08-06T16:58:06.458381Z","iopub.status.idle":"2024-08-06T16:58:41.756536Z","shell.execute_reply.started":"2024-08-06T16:58:06.458318Z","shell.execute_reply":"2024-08-06T16:58:41.755141Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: geopy in /opt/conda/lib/python3.10/site-packages (2.4.1)\nRequirement already satisfied: geographiclib<3,>=1.52 in /opt/conda/lib/python3.10/site-packages (from geopy) (2.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install numpy","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:58:56.229357Z","iopub.execute_input":"2024-08-06T16:58:56.229771Z","iopub.status.idle":"2024-08-06T16:59:31.527698Z","shell.execute_reply.started":"2024-08-06T16:58:56.229735Z","shell.execute_reply":"2024-08-06T16:59:31.526269Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport csv\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport json\nfrom geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter\nimport numpy as np\nimport seaborn as sns","metadata":{"id":"VXUEW0VrF7XW","execution":{"iopub.status.busy":"2024-08-06T16:59:36.029794Z","iopub.execute_input":"2024-08-06T16:59:36.031299Z","iopub.status.idle":"2024-08-06T16:59:37.867166Z","shell.execute_reply.started":"2024-08-06T16:59:36.031227Z","shell.execute_reply":"2024-08-06T16:59:37.865983Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## 3\\. Exploração de dados","metadata":{"id":"irQxHW1zGkdZ"}},{"cell_type":"markdown","source":"#### 3.1 Coleta de Dados","metadata":{}},{"cell_type":"markdown","source":"O dado bruto é um arquivo do tipo `JSON` com uma lista de instâncias de entregas. Cada instância representa um conjunto de **entregas** que devem ser realizadas pelos **veículos** do **hub** regional. Exemplo:","metadata":{}},{"cell_type":"code","source":"[\n  {\n    \"name\": \"cvrp-0-df-0\",\n    \"region\": \"df-0\",\n    \"origin\": {\"lng\": -47.802664728268745, \"lat\": -15.657013854445248},\n    \"vehicle_capacity\": 180,\n    \"deliveries\": [\n      {\n        \"id\": \"ed0993f8cc70d998342f38ee827176dc\",\n        \"point\": {\"lng\": -47.7496622016347, \"lat\": -15.65879313293694},\n        \"size\": 10\n      },\n      {\n        \"id\": \"c7220154adc7a3def8f0b2b8a42677a9\",\n        \"point\": {\"lng\": -47.75887552060412, \"lat\": -15.651440380492554},\n        \"size\": 10\n      },\n      ...\n    ]\n  }\n]\n...\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Onde:\n\n - **name**: uma `string` com o nome único da instância;\n - **region**: uma `string` com o nome único da região do **hub**;\n - **origin**: um `dict` com a latitude e longitude da região do **hub**;\n - **vehicle_capacity**: um `int` com a soma da capacidade de carga dos **veículos** do **hub**;\n - **deliveries**: uma `list` de `dict` com as **entregas** que devem ser realizadas.","metadata":{}},{"cell_type":"markdown","source":"Sendo que:\n\n - **id**: uma `string` com o id único da **entrega**;\n - **point**: um `dict` com a latitude e longitude da **entrega**;\n - **size**: um `int` com o tamanho ou a carga que a **entrega** ocupa no **veículo**.","metadata":{}},{"cell_type":"markdown","source":"O dado bruto está disponível para download neste [link](https://github.com/andre-marcos-perez/ebac-course-utils/blob/main/dataset/deliveries.json). Vamos realizar o seu download num arquivo `JSON` com o nome `deliveries.json`.\n","metadata":{}},{"cell_type":"code","source":"!wget -q \"https://raw.githubusercontent.com/andre-marcos-perez/ebac-course-utils/main/dataset/deliveries.json\" -O deliveries.json ","metadata":{"id":"lxLj8e0GHAnr","execution":{"iopub.status.busy":"2024-08-06T17:37:28.636580Z","iopub.execute_input":"2024-08-06T17:37:28.637130Z","iopub.status.idle":"2024-08-06T17:37:49.841811Z","shell.execute_reply.started":"2024-08-06T17:37:28.637077Z","shell.execute_reply":"2024-08-06T17:37:49.840095Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"!rm deliveries.json","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:42:14.058182Z","iopub.execute_input":"2024-08-06T17:42:14.058784Z","iopub.status.idle":"2024-08-06T17:42:15.244069Z","shell.execute_reply.started":"2024-08-06T17:42:14.058735Z","shell.execute_reply":"2024-08-06T17:42:15.242553Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"rm: cannot remove 'deliveries.json': No such file or directory\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm 'deliveries-geodata.csv'","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:37:18.183374Z","iopub.execute_input":"2024-08-06T17:37:18.183926Z","iopub.status.idle":"2024-08-06T17:37:19.349709Z","shell.execute_reply.started":"2024-08-06T17:37:18.183820Z","shell.execute_reply":"2024-08-06T17:37:19.347820Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Vamos carregar os dados do arquivo em um dicionário Python chamado `data` e depois criar um dataframe chamado `deliveries_df` a partir dele:","metadata":{}},{"cell_type":"code","source":"# importar arquivo csv para dataframe\n\nwith open('deliveries.json', mode='r', encoding='utf8') as file:\n  data = json.load(file)\n\ndeliveries_df = pd.DataFrame(data)\ndeliveries_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:39:34.734963Z","iopub.execute_input":"2024-08-06T17:39:34.735417Z","iopub.status.idle":"2024-08-06T17:39:34.840894Z","shell.execute_reply.started":"2024-08-06T17:39:34.735378Z","shell.execute_reply":"2024-08-06T17:39:34.838704Z"},"trusted":true},"execution_count":15,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# importar arquivo csv para dataframe\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeliveries.json\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 4\u001b[0m   data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m deliveries_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[1;32m      7\u001b[0m deliveries_df\u001b[38;5;241m.\u001b[39mhead()\n","File \u001b[0;32m/opt/conda/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n","File \u001b[0;32m/opt/conda/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n","File \u001b[0;32m/opt/conda/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"],"ename":"JSONDecodeError","evalue":"Expecting value: line 1 column 1 (char 0)","output_type":"error"}]},{"cell_type":"code","source":"# Visualizar tamanho do dataframe\n//len(data)\nlen(deliveries_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vamos então explorar um exemplo:","metadata":{}},{"cell_type":"code","source":"# Passar dados do dicionário de índice 0 para a variável exemplo\nexample = data[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizar colunas do exemplo\nprint(example.keys())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizar coluna name\nexample['name']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizar coluna region\nexample['region']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - **Coluna**: origin\n \nObserve que a coluna `origin` contém dados `nested` ou aninhados na estrutura do JSON. Vamos normalizar essa coluna usando uma operação chamada `flatten` ou achatamento, que transforma cada chave do JSON em uma nova coluna. Depois vamos juntar os dados com o conjunto principal e por fim, reorganizar as colunas e renomear os campos de latitude e longitude dos hubs.","metadata":{}},{"cell_type":"code","source":"# Normalizar a coluna aninhada origin\n\nhub_origin_df = pd.json_normalize(deliveries_df[\"origin\"])\n\n# Juntar os dados achatados com o conjunto principal\ndeliveries_df = pd.merge(left=deliveries_df, right=hub_origin_df, how='inner', left_index=True, right_index=True)\ndeliveries_df = deliveries_df.drop(\"origin\", axis=1)\n\n# Reorganizar e renomear colunas\ndeliveries_df = deliveries_df[[\"name\", \"region\", \"lng\", \"lat\", \"vehicle_capacity\", \"deliveries\"]]\ndeliveries_df.rename(columns={\"lng\": \"hub_lng\", \"lat\": \"hub_lat\"}, inplace=True)\n\ndeliveries_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T13:20:01.252933Z","iopub.execute_input":"2024-08-06T13:20:01.254411Z","iopub.status.idle":"2024-08-06T13:20:01.324868Z","shell.execute_reply.started":"2024-08-06T13:20:01.254301Z","shell.execute_reply":"2024-08-06T13:20:01.322828Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - **Coluna**: deliveries\n \nNote que a coluna `deliveries` contem dados uma lista de dados `nested` ou aninhados na estrutura do JSON. Vamos normalizar a coluna com uma operação conhecida como `explode` ou explosão que transforma cada elemento da lista em uma linha. Por fim, faremos os `flatten` ou achatamento do resultado coluna:","metadata":{}},{"cell_type":"code","source":"# Normalizar a coluna aninhada deliveries com a função explode que expande o campo\ndeliveries_exploded_df = deliveries_df[[\"deliveries\"]].explode(\"deliveries\")\ndeliveries_exploded_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalizar os dados de deliveries\ndeliveries_normalized_df = pd.concat([\n  pd.DataFrame(deliveries_exploded_df[\"deliveries\"].apply(lambda record: record[\"size\"])).rename(columns={\"deliveries\": \"delivery_size\"}),\n  pd.DataFrame(deliveries_exploded_df[\"deliveries\"].apply(lambda record: record[\"point\"][\"lng\"])).rename(columns={\"deliveries\": \"delivery_lng\"}),\n  pd.DataFrame(deliveries_exploded_df[\"deliveries\"].apply(lambda record: record[\"point\"][\"lat\"])).rename(columns={\"deliveries\": \"delivery_lat\"}),\n], axis= 1)\ndeliveries_normalized_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Com o dados explodidos, vamos normaliza-los para combina-los ao conjunto de dados principal:","metadata":{}},{"cell_type":"code","source":"# Visualizar o tamanho do dataframe deliveries_exploded_df\nlen(deliveries_exploded_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizar o tamanho do dataframe deliveries_df\nlen(deliveries_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Juntar os dados normalizados de entregas (deliveries) com a tabela principal\ndeliveries_df = deliveries_df.drop(\"deliveries\", axis=1)\ndeliveries_df = pd.merge(left=deliveries_df, right=deliveries_normalized_df, how='right', left_index=True, right_index=True)\ndeliveries_df.reset_index(inplace=True, drop=True)\ndeliveries_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizar o tamanho do dataframe deliveries_df a fim de se certificar que a tabela continua do mesmo tamanho\nlen(deliveries_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Agora vamos analisar a estrutura do nosso conjunto de dados reorganizada.","metadata":{}},{"cell_type":"markdown","source":"#### 3.2 Estrutura","metadata":{}},{"cell_type":"code","source":"# Visualizar a estrutura\ndeliveries_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizar as colunas do dataframe\ndeliveries_df.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizar os índices\ndeliveries_df.index","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizar as informações adicionais do dataframe\ndeliveries_df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3 Schema","metadata":{}},{"cell_type":"code","source":"# Visualizar 5 linhas do dataframe\ndeliveries_df.head(n=5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - Colunas e seus respectivos tipos de dados.","metadata":{}},{"cell_type":"code","source":"# Visualizar tipos de dados\ndeliveries_df.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - Atributos **categóricos**.","metadata":{}},{"cell_type":"code","source":"# Visualizar atributos categóricos\ndeliveries_df.select_dtypes(\"object\").describe().transpose()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - Atributos **numéricos**.","metadata":{}},{"cell_type":"code","source":"# Visualizar atributos numéricos\ndeliveries_df.drop([\"name\", \"region\"], axis=1).select_dtypes('int64').describe().transpose()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.4 Dados faltantes","metadata":{}},{"cell_type":"markdown","source":"Podem ser eles:\n\n - Vazios (`\"\"`);\n - Nulos (`None`);\n - Não disponíveis ou aplicaveis (`na`, `NA`, etc.);\n - Não numérico (`nan`, `NaN`, `NAN`, etc).","metadata":{}},{"cell_type":"code","source":"# Visualizar dados faltantes\ndeliveries_df.isna().any()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4\\. Manipulação","metadata":{"id":"98hexQTyJS9I"}},{"cell_type":"markdown","source":"#### 4.1 Enriquecimento:","metadata":{}},{"cell_type":"markdown","source":"- **Geocodificação reversa do hub**\n\nNesta etapa será realizada a **geocodificação reversa** dos hubs de entrega com intuíto de enriquecer os dados trazendo a informações relevantes como nome da cidade e bairro.\n\nA **geocodificação** é o processo que transforma uma localização descrita por um texto (endereço, nome do local, etc.) em sua respectiva coodernada geográfica (latitude e longitude). A **geocodificação reversa** faz o oposto, transforma uma coordenada geográfica de um local em suas respectivas descrições textuais.","metadata":{}},{"cell_type":"code","source":"# Passar dados dos hubs para um dataframe exclusivo e visualizar\nhub_df = deliveries_df[[\"region\", \"hub_lng\", \"hub_lat\"]]\nhub_df = hub_df.drop_duplicates().sort_values(by=\"region\").reset_index(drop=True)\nhub_df.head()","metadata":{"id":"DXU4Ee0QJS9Q","execution":{"iopub.status.busy":"2024-08-05T20:49:02.541136Z","iopub.execute_input":"2024-08-05T20:49:02.541631Z","iopub.status.idle":"2024-08-05T20:49:03.068135Z","shell.execute_reply.started":"2024-08-05T20:49:02.541597Z","shell.execute_reply":"2024-08-05T20:49:03.066239Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Empresas como Google, Bing e Yahoo! fornecem **geocodificação** como serviço (e cobram por isso). Existe uma projeto *open source* chamado de [OpenStreetMap](https://www.openstreetmap.org/) que mantem um serviço gratuito de geocodificação chamado [Nominatim](https://nominatim.org/), serviço este que apresenta como limitação a quantia de [uma única consuta por segundo](https://operations.osmfoundation.org/policies/nominatim/). Vamos utilizá-lo através do pacote Python `geopy` para fazer a operação reversa e enriquecer o nosso DataFrame principal.","metadata":{}},{"cell_type":"markdown","source":"Agora vamos aplicar a geocodificação nas coordenadas das três regiões e extrair informações de **cidade** e **bairro**.","metadata":{}},{"cell_type":"code","source":"# Definir um usuario e um limite de tempo para acessar o serviço Nominatim a fim de respeitar o limite de uso\ngeolocator = Nominatim(user_agent=\"ebac_geocoder\")\ngeocoder = RateLimiter(geolocator.reverse, min_delay_seconds=1)\n\n# Fazer operação de geocodificação reversa nas linhas do dataframe\nhub_df[\"coordinates\"] = hub_df[\"hub_lat\"].astype(str)  + \", \" + hub_df[\"hub_lng\"].astype(str) \nhub_df[\"geodata\"] = hub_df[\"coordinates\"].apply(geocoder)\n\nhub_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Passar os dados do dataframe hub_df para um geodataframe normalizando a coluna de coordenadas criada pela geocodificação reversa\nhub_geodata_df = pd.json_normalize(hub_df[\"geodata\"].apply(lambda data: data.raw))\nhub_geodata_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Organizar as colunas com as informações extraídas selecionando apenas o bairro (suburb) e a cidade (city)\nhub_geodata_df = hub_geodata_df[[\"address.town\", \"address.suburb\", \"address.city\"]]\nhub_geodata_df.rename(columns={\"address.town\": \"hub_town\", \"address.suburb\": \"hub_suburb\", \"address.city\": \"hub_city\"}, inplace=True)\nhub_geodata_df[\"hub_city\"] = np.where(hub_geodata_df[\"hub_city\"].notna(), hub_geodata_df[\"hub_city\"], hub_geodata_df[\"hub_town\"])\nhub_geodata_df[\"hub_suburb\"] = np.where(hub_geodata_df[\"hub_suburb\"].notna(), hub_geodata_df[\"hub_suburb\"], hub_geodata_df[\"hub_city\"])\nhub_geodata_df = hub_geodata_df.drop(\"hub_town\", axis=1)\n\nhub.geodata_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"O DataFrame `hub_geodata_df` com as informações de **cidade** e **bairro** é então combinado ao DataFrame principal `deliveries_df`, enriquecendo assim o dado.","metadata":{}},{"cell_type":"code","source":"# Devolver o hub_geodata_df para o dataframe principal\nhub_df = pd.merge(left=hub_df, right=hub_geodata_df, left_index=True, right_index=True)\nhub_df = hub_df[[\"region\", \"hub_suburb\", \"hub_city\"]]\nhub_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mesclar as informações do dataframe hub reestruturadas com o dataframe de entregas (deliveries_df)\ndeliveries_df = pd.merge(left=deliveries_df, right=hub_df, how=\"inner\", on=\"region\")\ndeliveries_df = deliveries_df[[\"name\", \"region\", \"hub_lng\", \"hub_lat\", \"hub_city\", \"hub_suburb\", \"vehicle_capacity\", \"delivery_size\", \"delivery_lng\", \"delivery_lat\"]]\ndeliveries_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Enquanto o **hub** contem apenas **3** geolocalizações distintas, as **entregas** somam o total de **636.149**, o que levaria em torno de 7 dias para serem consultadas no servidor do Nominatim, dada a restrição de uma consulta por segundo. Por isso para as entregas será feita a inclusão dos dados de Geocodificação reversa já processados anteriormente pela Ebac e disponibilizadas em um arquivo csv a partir do endereço abaixo:","metadata":{}},{"cell_type":"code","source":"!wget -q \"https://raw.githubusercontent.com/andre-marcos-perez/ebac-course-utils/main/dataset/deliveries-geodata.csv\" -O deliveries-geodata.csv ","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:00:42.011369Z","iopub.execute_input":"2024-08-06T17:00:42.011861Z","iopub.status.idle":"2024-08-06T17:01:03.212577Z","shell.execute_reply.started":"2024-08-06T17:00:42.011822Z","shell.execute_reply":"2024-08-06T17:01:03.210739Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Abrir arquivo com a geocodificação reversa das entregas já processadas e passando para um dataframe\ndeliveries_geodata_df = pd.read_csv(\"deliveries-geodata.csv\")\ndeliveries_geodata_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:01:16.843575Z","iopub.execute_input":"2024-08-06T17:01:16.844085Z","iopub.status.idle":"2024-08-06T17:01:17.471093Z","shell.execute_reply.started":"2024-08-06T17:01:16.844034Z","shell.execute_reply":"2024-08-06T17:01:17.469097Z"},"trusted":true},"execution_count":9,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Abrir arquivo com a geocodificação reversa das entregas já processadas e passando para um dataframe\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m deliveries_geodata_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeliveries-geodata.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m deliveries_geodata_df\u001b[38;5;241m.\u001b[39mhead()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n","File \u001b[0;32mparsers.pyx:581\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"],"ename":"EmptyDataError","evalue":"No columns to parse from file","output_type":"error"}]},{"cell_type":"code","source":"# Juntar dataframes\ndeliveries_df = pd.merge(left=deliveries_df, right=deliveries_geodata_df[[\"delivery_city\", \"delivery_suburb\"]], how=\"inner\", left_index=True, right_index=True)\n\ndeliveries_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.2 Qualidade\n\nAqui será analisada a qualidade dos dados para de avaliar a consistência do schema.","metadata":{}},{"cell_type":"code","source":"# Visualizar informações sobre o dataframe deliveries_df\ndeliveries_df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verificar dados faltantes\ndeliveries_df.isna().any()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Verificação dos dados de Geocodificação reversa","metadata":{}},{"cell_type":"code","source":"# Proporção (%) de nomes faltantes de cidades das entregas (delivery_city)\n100 * (deliveries_df[\"delivery_city\"].isna().sum() / len(deliveries_df))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Proporção (%) de nomes faltantes de bairros das entregas (delivery_suburb)\n100 * (deliveries_df[\"delivery_suburb\"].isna().sum() / len(deliveries_df))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Valores exclusivos de cidades de entregas (delivery_city)\nprop_df = deliveries_df[[\"delivery_city\"]].value_counts() / len(deliveries_df)\nprop_df.sort_values(ascending=False).head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Valores exclusivos de bairros de entregas (delivery_suburb)\nprop_df = deliveries_df[[\"delivery_suburb\"]].value_counts() / len(deliveries_df)\nprop_df.sort_values(ascending=False).head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5\\. Visualização","metadata":{"id":"KSgjP--1JS9R"}},{"cell_type":"code","source":"# faça o código de visualização de dados:\n#\n# - produza pelo menos duas visualizações;\n# - adicione um pequeno texto com os insights encontrados;\n# - etc.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5.1 Mapa de entregas por região\n\nVamos a partir daqui criar o mapa utilizando o pacote Python Geopandas ([link](https://geopandas.org/) da documentação) para visualizar as coordenadas dos **hubs** e das **entregas** no mapa do Distrito Federal, segmentados pela região dos **hubs**.","metadata":{}},{"cell_type":"markdown","source":" - **Mapa do Distrito Federal**\n \n Agora vamos fazer o download dos dados do mapa do Distrito Federal do site oficial do IBGE através do seguinte [link](https://www.ibge.gov.br/geociencias/cartas-e-mapas/bases-cartograficas-continuas) para criar o DataFrame `mapa`. Note a coluna `geometry`.","metadata":{}},{"cell_type":"code","source":"# Baixar os arquivos de shapefile do mapa do Distrito Federal pelo site do IBGE\n!wget -q \"https://geoftp.ibge.gov.br/cartas_e_mapas/bases_cartograficas_continuas/bc100/go_df/versao2016/shapefile/bc100_go_df_shp.zip\" -O distrito-federal.zip\n\n# Descompactar e destacar os dois arquivos principais\n!unzip -q distrito-federal.zip -d ./maps\n!cp ./maps/LIM_Unidade_Federacao_A.shp ./df.shp\n!cp ./maps/LIM_Unidade_Federacao_A.shx ./df.shx","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ler arquivos shapefile do Distrito Federal\nmapa = gpd.read_file('df.shp')\nmapa = mapa.loc[[0]]\nmapa.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - **Mapa dos Hubs**\n \n Vamos criar o DataFrame `geo_hub_df` através do DataFrame `deliveries_df`. Note a nova coluna `geometry`.","metadata":{}},{"cell_type":"code","source":"# Criar um geo dataframe dos hubs\nhub_df = deliveries_df[[\"region\", \"hub_lng\", \"hub_lat\"]].drop_duplicates().reset_index(drop=True)\ngeo_hub_df = gpd.GeoDataFrame(hub_df, geometry=gpd.points_from_xy(hub_df[\"hub_lng\"], hub_df[\"hub_lat\"]))\ngeo_hub_df.head()","metadata":{"id":"Jlj3ACWCJS9R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - **Mapa das Entregas**\n \n Vamos criar o DataFrame `geo_deliveries_df` através do DataFrame `deliveries_df`. Note a nova coluna `geometry`.","metadata":{}},{"cell_type":"code","source":"# Criar um geo dataframe das entregas\ngeo_deliveries_df = gpd.GeoDataFrame(deliveries_df, geometry=gpd.points_from_xy(deliveries_df[\"delivery_lng\"], deliveries_df[\"delivery_lat\"]))\ngeo_deliveries_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - **Visualização**\n \n Agora vamos definir as propriedades necessárias para gerar o gráfico do mapa.","metadata":{}},{"cell_type":"code","source":"# Criar o plot vazio\nfig, ax = plt.subplots(figsize = (50/2.54, 50/2.54))\n\n# Plot mapa do Distrito Federal\nmapa.plot(ax=ax, alpha=0.4, color=\"lightgrey\")\n\n# Plot das entregas\ngeo_deliveries_df.query(\"region == 'df-0'\").plot(ax=ax, markersize=1, color=\"red\", label=\"df-0\")\ngeo_deliveries_df.query(\"region == 'df-1'\").plot(ax=ax, markersize=1, color=\"blue\", label=\"df-1\")\ngeo_deliveries_df.query(\"region == 'df-2'\").plot(ax=ax, markersize=1, color=\"seagreen\", label=\"df-2\")\n\n# Plot dos hubs\ngeo_hub_df.plot(ax=ax, markersize=30, marker=\"x\", color=\"black\", label=\"hub\")\n\n# Plot da legenda\nplt.title(\"Entregas no Distrito Federal por Região\", fontdict={\"fontsize\": 16})\nlgnd = plt.legend(prop={\"size\": 15})\nfor handle in lgnd.legend_handles:\n    handle.set_sizes([50])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n\nObservando o gráfico, é possível notar que os **hubs** estão bem centralizados em relação às entregas feitas a partir deles. A distribuição foi bem estruturada. No **hub** `df0`, as entregas são mais distantes e espalhadas, enquanto no **hub** `df1` elas são mais centralizadas e densas. O **hub** `df2` apresenta uma alta densidade de entregas nas proximidades, mas também possui algumas entregas mais distantes, sugerindo uma localidade com características híbridas, combinando aspectos dos hubs `df0` e `df1`.","metadata":{}},{"cell_type":"markdown","source":"#### 5.2 Gráfico de entregas por região\n\nVamos a partir daqui criar um gráfico de barras utilizando a biblioteca Seaborn com o ituito mostrar a proporção de entregas em cada região.","metadata":{}},{"cell_type":"markdown","source":" - **Agregação**:","metadata":{}},{"cell_type":"code","source":"# Criar um DataFrame com as colunas 'region' e 'vehicle_capacity', calculando a proporção de entregas por região\ndata = pd.DataFrame(deliveries_df[[\"region\", \"vehicle_capacity\"]].value_counts(normalize=True)).reset_index()\ndata.rename(columns={0: \"region_percent\"}, inplace=True)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - **Visualização**:","metadata":{}},{"cell_type":"code","source":"# Configurar o estilo do gráfico e visualizar\nwith sns.axes_style(\"whitegrid\"):\n    chart = sns.barplot(data=data, x='region', y=\"region_percent\", ci=None, palette=\"pastel\")\n    chart.set(title=\"Proporção de entregas por região\", xlabel=\"Região\", ylabel=\"Proporção\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:","metadata":{}}]}